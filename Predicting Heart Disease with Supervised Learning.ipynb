{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport sklearn as sl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting Heart Disease with Supervised Learning techniques ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This analysis uses data taken from the 1988 heart disease dataset, as described [here](https://www.kaggle.com/johnsmith88/heart-disease-dataset).","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"full_data = pd.read_csv('../input/heart-disease-dataset/heart.csv')\n\nnum_features = ['age','trestbps','chol','restecg','thalach','oldpeak']\ncat_features = ['sex','cp','fbs','exang','ca','thal','slope','restecg']\nfull_data = full_data[['sex','cp','fbs','exang','slope','restecg','ca','thal','age','trestbps','chol','thalach','oldpeak','target']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MF_pairplot = sns.pairplot(full_data[['age','trestbps','chol','thalach','oldpeak','target']], hue='target', kind ='reg', height = 4)\nMF_pairplot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a lot of overlap between the classes for many of the numerical features, suggesting that the use of non-linear classification algorithms such as k-Nearest Neighbours, Decision Trees and Random Forests would be more suitable for performing classification. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Model Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We begin by removing NaN elements from the dataset and splitting into training and test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfull_data = full_data.dropna()\ny = full_data['target']\nX = full_data.drop('target', axis = 1)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state =42) \n\nprint(y_train.shape)\nprint(y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the purposes of this investigation, numerical features will be scaled using StandardScaler and catagorical features will be transformed using a One-Hot Encoder, before applying a Principle Component Analysis (PCA). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\ncat_column_tran = ColumnTransformer([('ohe',OneHotEncoder(sparse=False), slice(0,8,1))], remainder='passthrough')\nnum_column_tran = ColumnTransformer([('ss',StandardScaler(),slice(9,13,1))], remainder='passthrough')\npreprocess = Pipeline([('Cat_tran',cat_column_tran),('Num_tran',num_column_tran),('PCA',PCA(0.95))])\n\nprocessed_data = preprocess.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### k-Nearest Neighbours ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing a grid search to obtain the best value for k:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparams_KNN = dict(n_neighbors = range(1,10))\ngrid_search_KNN = GridSearchCV(KNN, param_grid = params_KNN, cv =4, scoring='recall')\ngrid_search_KNN.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN_best_k = grid_search_KNN.best_params_['n_neighbors']\nprint(\"For a k-Nearest Neighbors model, the optimal value of k is \"+str(KNN_best_k))\nKNN_df = pd.DataFrame(grid_search_KNN.cv_results_)\nfig_KNN = plt.figure(figsize=(12,9))\nplt.plot(KNN_df['param_n_neighbors'],KNN_df['mean_test_score'],'b-o')\nplt.xlim(0,10)\nplt.ylim(0.5,1.0)\nplt.xlabel('k')\nplt.ylabel('Mean recall over 4 cross-validation sets')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Decision Tree","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The grid search is used to find the optimal number of layers, with maximum number of features at each node set to 1 to prevent overfitting. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier(max_features = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_DT = dict(max_depth = range(2,30))\ngrid_search_DT = GridSearchCV(DT, param_grid = params_DT, cv = 4, scoring='recall')\ngrid_search_DT.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DT_best_layers = grid_search_DT.best_params_['max_depth']\nprint(\"For a Decision Tree model, the optimal number of layers is \"+str(DT_best_layers))\nDT_df = pd.DataFrame(grid_search_DT.cv_results_)\nfig = plt.figure(figsize=(12,9))\nplt.plot(DT_df['param_max_depth'],DT_df['mean_test_score'],'g-o')\nplt.xlim(0,30)\nplt.ylim(0.65,1.0)\nplt.xlabel('Maximum layers')\nplt.ylabel('Mean recall over 4 cross-validation sets')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Random Forest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We use the best number of layers from the single Decision Tree model to train the forest, using a grid search to find the optimal number of trees in the forest. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(max_depth = DT_best_layers, max_features = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_RF = dict(n_estimators = range(1,50))\ngrid_search_RF = GridSearchCV(RF, param_grid = params_RF, cv = 4, scoring='recall')\ngrid_search_RF.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF_best_estimators = grid_search_RF.best_params_['n_estimators']\nprint(\"For a Random Forest, the optimal number of estimators is \"+str(RF_best_estimators))\nRF_df = pd.DataFrame(grid_search_RF.cv_results_)\nfig = plt.figure(figsize=(30,9))\nplt.plot(RF_df['param_n_estimators'],RF_df['mean_test_score'],'r-o')\nplt.xlim(0,50)\nplt.ylim(0.65,1.0)\nplt.xlabel('Number of estimators')\nplt.ylabel('Mean recall over 4 cross-validation sets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of the models on the test set ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score, precision_score, accuracy_score\n\nKNN_final = grid_search_KNN.best_estimator_\nKNN_final.fit(processed_data, y_train)\n\nDT_final = grid_search_DT.best_estimator_\nDT_final.fit(processed_data, y_train)\n\nRF_final = grid_search_RF.best_estimator_\nRF_final.fit(processed_data, y_train)\n\npipelines = [KNN_final, DT_final, RF_final]\n\nbest_recall = 0.0\nbest_classifier = 0.0\nbest_pipeline = \"\"\n\npipe_dict = {0:'k-Nearest Neighbours',1:'Decision Tree',2:'Random Forest'}\n\nfor i,model in enumerate(pipelines):\n    X_test_trans = preprocess.transform(X_test)\n    y_pred = model.predict(X_test_trans)\n    print(\"{} test recall: {}\".format(pipe_dict[i],recall_score(y_pred, y_test) ))\n    print(\"{} test precision: {}\".format(pipe_dict[i],precision_score(y_pred, y_test) ))\n    print(\"{} test accuracy: {}\".format(pipe_dict[i],accuracy_score(y_pred, y_test) ))\n    if recall_score(y_pred,y_test)>best_recall:\n        best_recall = recall_score(y_pred,y_test)\n        best_pipeline = model \n        best_classifer = i\n\nprint(\"Classifier with best recall: {}\".format(pipe_dict[best_classifier]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"This seems a little too good to be true, even despite my attempts to perform regularisation when training the algorithms. I am aware that the dataset is very small, particularly for these algorithms - this analysis is one of my first independent projects so I'd be particularly grateful for any advice and feedback from the community, particularly regarding parameter selection and coding style. Let me know your thoughts in the comments!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}